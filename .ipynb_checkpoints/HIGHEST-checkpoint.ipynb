{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         60000 non-null  int64 \n",
      " 1   comment    60000 non-null  object\n",
      " 2   subreddit  60000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv (\"train.csv\")\n",
    "df.info()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I think prestige points should not expire ever...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Whats going to happen with them if they will b...</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Anecdotal evidence is anecdotal. Clearly by “e...</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Look dude, with all due respect, your music is...</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hope he gets the doomhammer back!</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Trading for coaches has happened before</td>\n",
       "      <td>nfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Considering what the kid has already seen, did...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Nah clearly it's Tom Bombadil</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Time to go play some Elite Dangerous in VR I t...</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>https://i.imgur.com/DUdy0KL.jpg</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment        subreddit\n",
       "0   0  I think prestige points should not expire ever...  leagueoflegends\n",
       "1   1  Whats going to happen with them if they will b...           europe\n",
       "2   2  Anecdotal evidence is anecdotal. Clearly by “e...    gameofthrones\n",
       "3   3  Look dude, with all due respect, your music is...            Music\n",
       "4   4                  Hope he gets the doomhammer back!              wow\n",
       "5   5            Trading for coaches has happened before              nfl\n",
       "6   6  Considering what the kid has already seen, did...           movies\n",
       "7   7                      Nah clearly it's Tom Bombadil           movies\n",
       "8   8  Time to go play some Elite Dangerous in VR I t...            Music\n",
       "9   9                    https://i.imgur.com/DUdy0KL.jpg            funny"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the text\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleaning The Data\n",
    "# def clean_text(df):\n",
    "    \n",
    "#     all_comments = list()\n",
    "#     lines = df[\"comment\"].values.tolist()\n",
    "#     for text in lines:\n",
    "#         text = text.lower()\n",
    "        \n",
    "#         pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "#         text = pattern.sub('', text)\n",
    "        \n",
    "#         emoji = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#         text = emoji.sub(r'', text)\n",
    "        \n",
    "#         text = re.sub(r\"i'm\", \"i am\", text)\n",
    "#         text = re.sub(r\"he's\", \"he is\", text)\n",
    "#         text = re.sub(r\"she's\", \"she is\", text)\n",
    "#         text = re.sub(r\"that's\", \"that is\", text)        \n",
    "#         text = re.sub(r\"what's\", \"what is\", text)\n",
    "#         text = re.sub(r\"where's\", \"where is\", text) \n",
    "#         text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "#         text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "#         text = re.sub(r\"\\'re\", \" are\", text)\n",
    "#         text = re.sub(r\"\\'d\", \" would\", text)\n",
    "#         text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "#         text = re.sub(r\"won't\", \"will not\", text)\n",
    "#         text = re.sub(r\"don't\", \"do not\", text)\n",
    "#         text = re.sub(r\"did't\", \"did not\", text)\n",
    "#         text = re.sub(r\"can't\", \"can not\", text)\n",
    "#         text = re.sub(r\"it's\", \"it is\", text)\n",
    "#         text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "#         text = re.sub(r\"have't\", \"have not\", text)\n",
    "#         text = re.sub(r\"nt\", \"not\", text)\n",
    "        \n",
    "#         text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", '', text)\n",
    "        \n",
    "#         tokens = word_tokenize(text)\n",
    "        \n",
    "#         table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "#         stripped = [w.translate(table) for w in tokens]\n",
    "#         words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         stop_words.discard(\"not\")\n",
    "        \n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "#         words = ' '.join(words)\n",
    "        \n",
    "#         all_comments.append(words)\n",
    "#     return all_comments\n",
    "\n",
    "# all_comments = clean_text(df)\n",
    "# all_comments[0:4]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think prestig point not expir ever skin buy avail set durat exempl year releas anoth skin vault old one make also limitededit skin also pleas love god nt rereleas skin need grind prestig shop would suck everyon grind',\n",
       " 'what go happen refus asilum appeal']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"comment\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)\n",
    "    return all_reviews\n",
    "\n",
    "all_reviews = clean_text(df)\n",
    "all_reviews[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-z]+')\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "text_counts_m = cv.fit(all_reviews)\n",
    "text_counts = text_counts_m.transform(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 41857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts.shape\n",
    "# d = text_counts\n",
    "# filtered_sentence_test = [] \n",
    "# freq_count_limit_test = FreqDist()\n",
    "# lemmatizer_test=WordNetLemmatizer()\n",
    "# stop_words_test = set(stopwords.words('english'))\n",
    "\n",
    "# for i in d:\n",
    "#     comment_tokens_test = word_tokenize(i)\n",
    "#     for words in comment_tokens_test:\n",
    "#         if words not in stop_words_test: \n",
    "#             filtered_sentence_test.append(words) \n",
    "        \n",
    "#             limit_words_test = lemmatizer.lemmatize(words)\n",
    "# #     for word in root_words:\n",
    "#             freq_count_limit_test[limit_words_test.lower()]+=1\n",
    "# freq_count_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, df['subreddit'], test_size=0.09, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.48907407407407405\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

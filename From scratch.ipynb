{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    \n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) \n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) \n",
    "    cleaned_str=cleaned_str.lower() \n",
    "    \n",
    "    return cleaned_str "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes # Constructor is sinply passed with unique number of classes of the training set\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "     \n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "          \n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "    def fit(self,dataset,labels):\n",
    "        \n",
    "    \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        #only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "        \n",
    "#         if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "#         if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat] #filter all examples of category == cat\n",
    "            \n",
    "            #get examples preprocessed\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            #now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "            \n",
    "                \n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            #Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            #Calculating total counts of all the words of each class \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "            \n",
    "            #get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        \n",
    "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #computing denominator value                                      \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "      \n",
    "        \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "                                             \n",
    "                                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
    "        \n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
    "                                             \n",
    "                \n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                #now get likelihood of this test_token word                              \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "                #remember why taking log? To prevent underflow!\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def predict(self,test_set):\n",
    "            \n",
    "       \n",
    "        predictions=[] #to store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            #preprocess the test example the same way we did for training set exampels                                  \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            #simply get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            #simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         60000 non-null  int64 \n",
      " 1   comment    60000 non-null  object\n",
      " 2   subreddit  60000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set=pd.read_csv (\"train.csv\")\n",
    "training_set.info()\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I think prestige points should not expire ever...</td>\n",
       "      <td>leagueoflegends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Whats going to happen with them if they will b...</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Anecdotal evidence is anecdotal. Clearly by “e...</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Look dude, with all due respect, your music is...</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hope he gets the doomhammer back!</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment        subreddit\n",
       "0   0  I think prestige points should not expire ever...  leagueoflegends\n",
       "1   1  Whats going to happen with them if they will b...           europe\n",
       "2   2  Anecdotal evidence is anecdotal. Clearly by “e...    gameofthrones\n",
       "3   3  Look dude, with all due respect, your music is...            Music\n",
       "4   4                  Hope he gets the doomhammer back!              wow"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on train data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  0.44472222222222224\n"
     ]
    }
   ],
   "source": [
    "#getting training set examples labels\n",
    "y_train=training_set['subreddit'].values\n",
    "x_train=training_set['comment'].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(x_train,y_train,\n",
    "                                                               shuffle=True,\n",
    "                                                               test_size=0.2\n",
    "                                                               ,random_state=42\n",
    "                                                               ,stratify=y_train)\n",
    "classes=np.unique(train_labels)\n",
    "\n",
    "# Training phase....\n",
    "\n",
    "nb=NaiveBayes(classes)\n",
    "nb.fit(train_data,train_labels)\n",
    "\n",
    "# Testing phase \n",
    "\n",
    "y_pred_train=nb.predict(test_data)\n",
    "test_acc=np.sum(y_pred_train==test_labels)/float(test_labels.shape[0])\n",
    "\n",
    "print (\"Test Set Accuracy: \",test_acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       20000 non-null  int64 \n",
      " 1   comment  20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 234.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set=pd.read_csv (\"test.csv\")\n",
    "testing_set.info()\n",
    "testing_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Holy shit a shot counter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It doesn't matter that it isn't hard to rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I find it funny that this is downvoted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>They are really getting ridicoulous with all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>He's Eden's best friend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment\n",
       "0   0                          Holy shit a shot counter.\n",
       "1   1  It doesn't matter that it isn't hard to rememb...\n",
       "2   2             I find it funny that this is downvoted\n",
       "3   3  They are really getting ridicoulous with all t...\n",
       "4   4                            He's Eden's best friend"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=testing_set.comment.values\n",
    "\n",
    "y_pred_test=nb.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = zip(list(range(len(y_pred_test))), y_pred_test)\n",
    "test_df = pd.DataFrame(submission, columns=['Id','Category'])\n",
    "test_df.to_csv('submission.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = x_train\n",
    "y = y_train\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cross_val_accuracy = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    nb.fit(X_train,y_train)\n",
    "    y_pred_cross_train=nb.predict(X_test)\n",
    "    test_cross_acc=np.sum(y_pred_cross_train==y_test)/float(y_test.shape[0])\n",
    "    main_cross_val_accuracy.append(test_cross_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4533333333333333,\n",
       " 0.44683333333333336,\n",
       " 0.45108333333333334,\n",
       " 0.45466666666666666,\n",
       " 0.4489166666666667]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_cross_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for selective classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think prestige points not expire ever skins buy available set duration exemple year release another skin vault old one making also limitededition skin also please love god nt rerelease skins need grind prestige shop would suck everyone grinded',\n",
       " 'whats going happen refused asilum appeal']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_text(training_set):\n",
    "    \n",
    "    all_comments = list()\n",
    "    lines = training_set[\"comment\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        \n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub(\"\", text)       \n",
    "    \n",
    "        \n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        \n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        \n",
    "        all_comments.append(words)\n",
    "    return all_comments\n",
    "\n",
    "all_comments = clean_text(training_set)\n",
    "all_comments[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'nt': 12955, 'like': 8953, 'one': 5971, 'people': 5884, 'get': 5839, 'would': 5745, 'think': 4498, 'time': 4385, 'game': 3900, 'even': 3839, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "c = all_comments\n",
    "filtered_sentence = [] \n",
    "freq_count_limit = FreqDist()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in c:\n",
    "    comment_tokens = word_tokenize(i)\n",
    "    \n",
    "    for words in comment_tokens:\n",
    "        if words not in stop_words: \n",
    "            filtered_sentence.append(words) \n",
    "        \n",
    "            limit_words = lemmatizer.lemmatize(words)\n",
    "#     for word in root_words:\n",
    "            freq_count_limit[limit_words.lower()]+=1\n",
    "freq_count_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freq_count_limit.plot(5,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing and transforming the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,1), max_features=30000, strip_accents='ascii')\n",
    "vect.fit(all_comments)\n",
    "vocabulaire = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 30000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vect.transform(all_comments)\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier(max_depth=15, random_state=42)\n",
    "clf_rf.fit(bag_of_words, training_set['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34563333333333335"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf.score(bag_of_words, training_set['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30675   , 0.30808333, 0.31591667, 0.31841667, 0.31941667])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_rf = cross_val_score(clf_rf, bag_of_words, training_set['subreddit'], cv=5)\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=15, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter=15, random_state=42)\n",
    "clf_lr.fit(bag_of_words, training_set['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5841666666666666"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.score(bag_of_words, training_set['subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\rezam\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.45608333, 0.4475    , 0.44933333, 0.45541667, 0.454     ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr = cross_val_score(clf_lr, bag_of_words, training_set['subreddit'], cv=5)\n",
    "scores_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from sklearn (just for comparison) between models & cleaning functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, training_set['subreddit'], \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "nb_sklearn = BernoulliNB()\n",
    "nb_sklearn.fit(X_train, y_train)\n",
    "predicted= nb_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes classification accuracy 0.41458333333333336\n"
     ]
    }
   ],
   "source": [
    "def accuracy_bnb(y_true, y_pred):\n",
    "    accuracy_bnb = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy_bnb\n",
    "print(\"Bernoulli Naive Bayes classification accuracy\", accuracy_bnb(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burnoulli Naive Bayes cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41758333, 0.3995    , 0.41175   , 0.41258333, 0.41266667])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr = cross_val_score(nb_sklearn, bag_of_words, training_set['subreddit'], cv=5)\n",
    "scores_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

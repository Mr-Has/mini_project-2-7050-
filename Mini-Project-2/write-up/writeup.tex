\documentclass[12pt]{report}





\begin{document}
	\title{Mini Project 2}
	
	\paragraph[]{Mini Project 2}
	\paragraph[]{}
	\textbf{Mohammad Hashtari, ID: 111284306}
	
	\paragraph[]{}
	\paragraph[]{}
	\paragraph{Abstract:}
	\paragraph{}
	This project involved development of a supervised classification model to predict on the test dataset, which contains 20k comments from different communities on Reddit platform.\\
	The goal was to provide the most accurate predictions of the communities for each test comment.
	There should be text pre-processing to clean the comments. There are many characters and symbols in the text of comments (especially  social media comments).\\
	The classifiers accept numbers and matrices, therefore the cleaned text must be processed and transformed into shapes that classifiers accept. \\
	Bernoulli Naive bayes is the classifiers mentioned by instruction to be used.
	Also two more classifiers to be added in order to compare the results.  
	As a competition, the best achieved result should be submitted in Kaggle platform to be compared with other students. 
	
	
	
	\paragraph{Introduction:}
	\paragraph{}
	The training data contained 2 main rows (comments, subreddit) and 60k columns. each column provided a sample comment of the related subreddit.\\
	There was no null value and the considered rows have object data types.
	The comments row of the train data set has passed to the cleaning function which process the text by different functions.\\
	The preprocess-string replace everything except alphabets with space (' '), and replace all the symbols and characters, Then it makes all the alphabets in a lowercase format. \\
	
	Bernoulli Naive Bayes classifiers is mentioned to be used in this project. This classifier has been implemented, and by using the k-fold validation the range of possible accuracy values have been calculated.\\
	As mentioned by (C.D. Manning, P. Raghavan and H. Schuetze (2008)), "BernoulliNB classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features." \\
	We move to the test data-set after all these processes. \\
	By looking at the data we can see that there is no null value and the comment row's data-type is object.
	By using the .values, we pass the values of the test (comment column) to the classifier.\\
	The submission function creates a csv file, containing the ID of the comments and the most reliable predict(subreddit) for each column. This submission file has been used for Kaggle competition.\\	
	The first option for selective classifiers is the Random Forest classifier. The training and test, data-sets have been passed to this classifier. Its noticeable that by incleasing the max-depth the accuracy score improved.\\	
	A cross-validation score has been placed in order to display the range of the possible accuracy. The results of the croos-validation are somehow close to the main achived result from Random Forest.\\
	The Second option for selective classifiers is the Logistic-Regression. As before the data-set have been passed to the classifier. \\
	The average of the cross-validation of other two classifiers were close to the single result achieved, but this was not the case in Logistic regression.\\
	In order to have a stable result of single run for each classifier, the random state 42 has been placed. \\	  		

	\paragraph{Related work:}
	\paragraph{}
	Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral. This method is really useful for today's businesses to understand the social sentiment of their brand, product or service while monitoring online conversations(Gupta, 2020).\\
	
	The traditional way of conducting the above task is based on sentiment lexicons . Sentiment lexicons can serve as a word-level basis to help analyze the sentiment of unlabeled documents for the discrete information such as polarities and strengths they contain(Catal and Nangir, 2017). Lexicon-based methods mainly exploited features such as the counts, the total strengths, and the maximum strengths of positive and negative words.\\
	Although such methods have been shown simple and efficient, they suffer from the imitation of existing sentiment lexicons(Chen et al., 2019).\\
	
	The study on 200k online product reviews demonstrates that, there is a benefit to use higher order n-grams beyond uni and bi-grams(Cui, H., Mittal, V. and Datar, M., 2006).\\
	But for the case of this project, there was no improvement in the results by experimenting higher n-grams.
	As mentioned by the authors this method is useful for large-scale data-sets. \\
	
	
	
	\paragraph{Dataset and setup:}
	\paragraph{}
	The data-set which as a 20-class classification problem with a balanced
	dataset has been downloaded from Kaggle platform. The data was provided in csv format.\\
	As mentioned in the instruction the data is balanced and comments are enclosed with quotes.
	Each column of the training data-set (60k columns) contain an ID, comment and the related community to the comment.\\
	There are two rows for the test data-set. The ID and the comment that should be predicted belongs to which community.
	By looking at the data after importing it, we can see that there is no null value and the data types of the comments and subreddits are objects. \\
	After cleaning the data-set by clean-text function, we use the word tokenizer to split the comments into tokens of individual words for further processing. Then by using the strip function, we try to bring the tokenized vocabularies to their root as much as possible.\\
	In the next step we eleminate all the unnecessary values which are not in English dictionary by using the stop word function. \\
	Then by appending the processed vocabularies to the defined list before, we prepared the data for further processes. \\
	
	
	\paragraph{}
	\paragraph{Proposed approach:}
	\paragraph{}
		Bernoulli Naive Bayes wa specified by the project to be used, and additional two more classifiers to be added to the project.\\
		The BernoulliNB was used through the project. Cross validation (k-fold) was implemented for the model. 
		One of the selective models is Random Forest. Observation is that by increasing the maximum depth (hyper parameter) of the model the accuracy increased as well, which was not correct. But the overall accuracy result was lower then the other two methods. \\
		There must be a consideration that we do not over fit the model by simple increasing the hyper parameter. \\
		Logistic Regression is the second selective model chosen for this project. The first impression is that the run time required for this model is much higher then others. By increasing the maximum iterations from 5 to 20 there was a large improvement (0.19) in accuracy, and by looking at the cross validation results we can notice that it has the same issue as Random Forest. But it seems that this method is not suitable for large scale data-sets. It runs, but it gives a notification that either re scale the data or increase the maximum iteration. Also other observation is that the results of the cross validation is always lower then the single run, and it can be due to the max-iteration, But this is not the case for other models. \\t...


	
	
	\paragraph{Results:}
	\paragraph{}
	\paragraph{Results of highest accuracy achieved:}
	\paragraph{}
	\begin{center}
		\begin{tabular}{ |p{3cm}||p{3cm}| }
			\hline
			\multicolumn{2}{|c|}{Highest Accuracy} \\
			\hline
			classifiers:& Accuracy:\\
			\hline
			-Bernoulli Naive Bayes (from scratch)   & 0.47462\\
			\hline
		\end{tabular}
	\end{center}
	
	\paragraph{}
	\paragraph{Results of single run for each model:}
	\paragraph{}
	
	\begin{center}
		\begin{tabular}{ |p{3cm}||p{3cm}| }
			\hline
			\multicolumn{2}{|c|}{Accuracy} \\
			\hline
			classifiers:& Accuracy:\\
			\hline
			-Bernoulli Naive Bayes (from scratch)   & 0.5183333333336\\
			-Random Forest&   0.3456333333333\\
			-Logistic Regression &0.4988166666666\\
			\hline
		\end{tabular}
	\end{center}
	\paragraph{}
	\paragraph{Results of cross validation for each model:}
	\paragraph{}
	\begin{center}
		\begin{tabular}{||c c c||} 
			\hline
			Bernoulli Naive Bayes & Random Forest & Logistic Regression\\ [0.5ex] 
			\hline\hline
			0.45333333333 & 0.30675 & 0.41966667\\ 
			\hline
			0.44683333333 & 0.30808333 & 0.41683333\\
			\hline
			0.45108333333 & 0.31591667 & 0.42291667\\
			\hline
			0.45466666666 & 0.31841667& 0.42075   \\
			\hline
			0.44891666666 & 0.31941667 & 0.42783333\\ [1ex] 
			\hline
		\end{tabular}
	\end{center}
		

	\paragraph{Discussion and Conclusion:}
	\paragraph{}
	The best take away is that before this project I had no knowledge about text processing and predictions on text, but now I have learned to use three classifiers and tune them to predict on processed text. \\
	As mentioned by (Brash, 2015) to overcome the over-fitting, we should optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data. we do this via k-fold cross-validation, therefore the results of the cross validation (Random Forest, Logistic Regression) indicate the fact that no matter how single run accuracy is high, the cross validation provide the most accurate results.\\
		

	\paragraph{Statement of Contributions:}
	\paragraph{}
	This project has been carried out individually, by the instructions provided by Prof. Brahim Chaib draa, and under the supervision of Dr. Amar Ali Bey.\\
	
	\paragraph{References:}
	\paragraph{}
	- C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\\
	
	- A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\\
	
	- V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with naive Bayes – Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\\
	
	- Cui, H., Mittal, V. and Datar, M., 2006, July. Comparative experiments on sentiment classification for online product reviews. In AAAI (Vol. 6, No. 1265-1270, p. 30).\\
	
	- Gupta, S., 2020. Sentiment Analysis: Concept, Analysis And Applications. [online] Medium. Available at: <https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17> [Accessed 13 July 2020].\\
	
	- Catal, C. and Nangir, M., 2017. A sentiment classification model based on multiple classifiers. Applied Soft Computing, 50, pp.135-141.\\
	
	- Chen, X., Rao, Y., Xie, H., Wang, F., Zhao, Y. and Yin, J., 2019. Sentiment Classification Using Negative and Intensive Sentiment Supplement Information. Data Science and Engineering, 4(2), pp.109-118.\\
	
	- overfitting, R., Equilibrium, B., Ooi, H. and Grigorev, A., 2020. Random Forest - How To Handle Overfitting. [online] Cross Validated. Available at: <https://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting> [Accessed 13 July 2020].\\
	
	- Javed, A., 2020. Naïve Bayes From Scratch Using Python Only— No Fancy Frameworks. [online] Medium. Available at: <https://towardsdatascience.com/na%C3%AFve-bayes-from-scratch-using-python-only-no-fancy-frameworks-a1904b37222d> [Accessed 13 July 2020].\\
	
	
\end{document}        

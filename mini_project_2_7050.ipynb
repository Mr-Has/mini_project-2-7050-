{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv (\"train.csv\")\n",
    "df.info()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    \n",
    "    all_comments = list()\n",
    "    lines = df[\"comment\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        \n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        \n",
    "        emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        text = emoji.sub(r'', text)\n",
    "        \n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)        \n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text) \n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"did't\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"have't\", \"have not\", text)\n",
    "        text = re.sub(r\"nt\", \"not\", text)\n",
    "        \n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        \n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        \n",
    "        all_comments.append(words)\n",
    "    return all_comments\n",
    "\n",
    "all_comments = clean_text(df)\n",
    "all_comments[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = all_comments\n",
    "filtered_sentence = [] \n",
    "freq_count_limit = FreqDist()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in c:\n",
    "    comment_tokens = word_tokenize(i)\n",
    "    for words in comment_tokens:\n",
    "        if words not in stop_words: \n",
    "            filtered_sentence.append(words) \n",
    "        \n",
    "            limit_words = lemmatizer.lemmatize(words)\n",
    "#     for word in root_words:\n",
    "            freq_count_limit[limit_words.lower()]+=1\n",
    "freq_count_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_limit.plot(20,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_limit.pop(\"nt\")\n",
    "freq_count_limit.pop(\"would\")\n",
    "freq_count_limit.pop(\"one\")\n",
    "freq_count_limit.pop(\"get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_limit.plot(20,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(freq_count_limit.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_cleaned_words = sorted(freq_count_limit.items(), key=lambda x: x[1], reverse=True)\n",
    "# len(sorted_cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_100_words = sorted_cleaned_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists = top_100_words\n",
    "# x, y = zip(*lists)\n",
    "# x=list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = RegexpTokenizer(r'[a-z]+')\n",
    "# cv = CountVectorizer(ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "# text_counts = cv.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-z]+')\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "text_counts2 = cv.fit_transform(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts2, df['subreddit'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv (\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(df_test):\n",
    "    \n",
    "#     all_comments_test = list()\n",
    "#     lines = df_test[\"comment\"].values.tolist()\n",
    "#     for text in lines:\n",
    "#         text = text.lower()\n",
    "        \n",
    "#         pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "#         text = pattern.sub('', text)\n",
    "        \n",
    "#         emoji = re.compile(\"[\"\n",
    "#                            u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            u\"\\U00002702-\\U000027B0\"\n",
    "#                            u\"\\U000024C2-\\U0001F251\"\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "#         text = emoji.sub(r'', text)\n",
    "        \n",
    "#         text = re.sub(r\"i'm\", \"i am\", text)\n",
    "#         text = re.sub(r\"he's\", \"he is\", text)\n",
    "#         text = re.sub(r\"she's\", \"she is\", text)\n",
    "#         text = re.sub(r\"that's\", \"that is\", text)        \n",
    "#         text = re.sub(r\"what's\", \"what is\", text)\n",
    "#         text = re.sub(r\"where's\", \"where is\", text) \n",
    "#         text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "#         text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "#         text = re.sub(r\"\\'re\", \" are\", text)\n",
    "#         text = re.sub(r\"\\'d\", \" would\", text)\n",
    "#         text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "#         text = re.sub(r\"won't\", \"will not\", text)\n",
    "#         text = re.sub(r\"don't\", \"do not\", text)\n",
    "#         text = re.sub(r\"did't\", \"did not\", text)\n",
    "#         text = re.sub(r\"can't\", \"can not\", text)\n",
    "#         text = re.sub(r\"it's\", \"it is\", text)\n",
    "#         text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "#         text = re.sub(r\"have't\", \"have not\", text)\n",
    "#         text = re.sub(r\"nt\", \"not\", text)\n",
    "        \n",
    "#         text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        \n",
    "#         tokens = word_tokenize(text)\n",
    "        \n",
    "#         table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "#         stripped = [w.translate(table) for w in tokens]\n",
    "#         words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         stop_words.discard(\"not\")\n",
    "        \n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "#         words = ' '.join(words)\n",
    "        \n",
    "#         all_comments_test.append(words)\n",
    "#     return all_comments_test\n",
    "\n",
    "# all_comments_test = clean_text(df_test)\n",
    "# all_comments_test[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = all_comments_test\n",
    "# filtered_sentence_test = [] \n",
    "# freq_count_limit_test = FreqDist()\n",
    "# lemmatizer_test=WordNetLemmatizer()\n",
    "# stop_words_test = set(stopwords.words('english'))\n",
    "\n",
    "# for i in d:\n",
    "#     comment_tokens_test = word_tokenize(i)\n",
    "#     for words in comment_tokens_test:\n",
    "#         if words not in stop_words_test: \n",
    "#             filtered_sentence_test.append(words) \n",
    "        \n",
    "#             limit_words_test = lemmatizer.lemmatize(words)\n",
    "# #     for word in root_words:\n",
    "#             freq_count_limit_test[limit_words_test.lower()]+=1\n",
    "# freq_count_limit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_test = RegexpTokenizer(r'[a-z]+')\n",
    "# cv_test = CountVectorizer(ngram_range = (1,1), tokenizer = token_test.tokenize)\n",
    "# text_counts_test = cv.fit_transform(all_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_counts_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     text_counts_test, df['subreddit'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdist_top500 = freq_count_limit.most_common(1000) #[-10:]\n",
    "# fdist_top500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotes_biogram = list(nltk.trigrams(freq_count_limit))\n",
    "# quotes_biogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('subreddit').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community_dummies = df['subreddit'].str.get_dummies(sep=' ')\n",
    "# community_dummies[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.comment,df., test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# v = CountVectorizer()\n",
    "# X_train_count = v.fit_transform(X_train.values)\n",
    "# X_train_count.toarray()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# model = MultinomialNB()\n",
    "# model.fit(X_train_count,y_train)\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # noinspection SpellCheckingInspection\n",
    "# class NaiveBayesClassifier(object):\n",
    "#     def __init__(self, n_gram=1, printing=False):\n",
    "#         self.prior = defaultdict(int)\n",
    "#         self.logprior = {}\n",
    "#         self.bigdoc = defaultdict(list)\n",
    "#         self.loglikelihoods = defaultdict(defaultdict)\n",
    "#         self.V = []\n",
    "#         self.n = n_gram\n",
    "\n",
    "#         training_set = all_comments\n",
    "#         training_labels = top_100_words\n",
    "        \n",
    "#     def compute_prior_and_bigdoc(self, training_set, training_labels):\n",
    "        \n",
    "#         for x, y in zip(training_set, training_labels):\n",
    "#             all_words = x.split(\" \")\n",
    "#             if self.n == 1:\n",
    "#                 grams = all_words\n",
    "#             else:\n",
    "#                 grams = self.words_to_grams(all_words)\n",
    "\n",
    "#             self.prior[y] += len(grams)\n",
    "#             self.bigdoc[y].append(x)\n",
    "\n",
    "#     def compute_vocabulary(self, documents):\n",
    "#         vocabulary = set()\n",
    "\n",
    "#         for doc in documents:\n",
    "#             for word in doc.split(\" \"):\n",
    "#                 vocabulary.add(word.lower())\n",
    "\n",
    "#         return vocabulary\n",
    "\n",
    "#     def count_word_in_classes(self):\n",
    "#         counts = {}\n",
    "#         for c in list(self.bigdoc.keys()):\n",
    "#             docs = self.bigdoc[c]\n",
    "#             counts[c] = defaultdict(int)\n",
    "#             for doc in docs:\n",
    "#                 words = doc.split(\" \")\n",
    "#                 for word in words:\n",
    "#                     counts[c][word] += 1\n",
    "\n",
    "#         return counts\n",
    "\n",
    "#     def train(self, training_set, training_labels, alpha=1):\n",
    "#         # Get number of documents\n",
    "#         N_doc = len(training_set)\n",
    "\n",
    "#         # Get vocabulary used in training set\n",
    "#         self.V = self.compute_vocabulary(training_set)\n",
    "\n",
    "#         # Create bigdoc\n",
    "#         for x, y in zip(training_set, training_labels):\n",
    "#             self.bigdoc[y].append(x)\n",
    "\n",
    "#         # Get set of all classes\n",
    "#         all_classes = set(training_labels)\n",
    "\n",
    "#         # Compute a dictionary with all word counts for each class\n",
    "#         self.word_count = self.count_word_in_classes()\n",
    "\n",
    "#         # For each class\n",
    "#         for c in all_classes:\n",
    "#             # Get number of documents for that class\n",
    "#             N_c = float(sum(training_labels == c))\n",
    "\n",
    "#             # Compute logprior for class\n",
    "#             self.logprior[c] = np.log(N_c / N_doc)\n",
    "\n",
    "#             # Calculate the sum of counts of words in current class\n",
    "#             total_count = 0\n",
    "#             for word in self.V:\n",
    "#                 total_count += self.word_count[c][word]\n",
    "\n",
    "#             # For every word, get the count and compute the log-likelihood for this class\n",
    "#             for word in self.V:\n",
    "#                 count = self.word_count[c][word]\n",
    "#                 self.loglikelihoods[c][word] = np.log((count + alpha) / (total_count + alpha * len(self.V)))\n",
    "\n",
    "#     def predict(self, test_doc):\n",
    "#         sums = {\n",
    "#             0: 0,\n",
    "#             1: 0,\n",
    "#         }\n",
    "#         for c in self.bigdoc.keys():\n",
    "#             sums[c] = self.logprior[c]\n",
    "#             words = test_doc.split(\" \")\n",
    "#             for word in words:\n",
    "#                if word in self.V:\n",
    "#                    sums[c] += self.loglikelihoods[c][word]\n",
    "\n",
    "#         return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBclassifier = NaiveBayesClassifier(n_gram=1)\n",
    "# NBclassifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = NBclassifier.predict(test)\n",
    "# print(np.exp(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

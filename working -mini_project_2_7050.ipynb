{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         60000 non-null  int64 \n",
      " 1   comment    60000 non-null  object\n",
      " 2   subreddit  60000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv (\"train.csv\")\n",
    "df.info()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think prestige poinots not expire ever skins buy available set duration exemple year release another skin vault old one making also limitededition skin also please love god not rerelease skins need grind prestige shop would suck everyone grinded',\n",
       " 'whats going happen refused asilum appeal',\n",
       " 'anecdotal evidence anecdotal clearly everyone meanot like people not',\n",
       " 'look dude due respect music people looks like carti either caught much flak maybe senot polite post inviting discussion instead capitalizing every impactful word post carti',\n",
       " 'hope gets doomhammer back',\n",
       " 'trading coaches happened',\n",
       " 'considering kid already seen nt figure would matter',\n",
       " 'nah clearly tom bombadil',\n",
       " 'time go play elite dangerous vr think thanks',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(df):\n",
    "    \n",
    "    all_comments = list()\n",
    "    lines = df[\"comment\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        \n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        \n",
    "        emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        text = emoji.sub(r'', text)\n",
    "        \n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)        \n",
    "        text = re.sub(r\"what's\", \"what is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text) \n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"don't\", \"do not\", text)\n",
    "        text = re.sub(r\"did't\", \"did not\", text)\n",
    "        text = re.sub(r\"can't\", \"can not\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "        text = re.sub(r\"have't\", \"have not\", text)\n",
    "        text = re.sub(r\"nt\", \"not\", text)\n",
    "        \n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", '', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        \n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        \n",
    "        all_comments.append(words)\n",
    "    return all_comments\n",
    "\n",
    "all_comments = clean_text(df)\n",
    "all_comments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'like': 8954, 'would': 7132, 'nt': 6837, 'one': 5971, 'people': 5884, 'get': 5839, 'think': 4498, 'time': 4385, 'game': 3900, 'even': 3839, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = all_comments\n",
    "# filtered_sentence = [] \n",
    "# freq_count_limit = FreqDist()\n",
    "# lemmatizer=WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# for i in c:\n",
    "#     comment_tokens = word_tokenize(i)\n",
    "#     for words in comment_tokens:\n",
    "#         if words not in stop_words: \n",
    "#             filtered_sentence.append(words) \n",
    "        \n",
    "#             limit_words = lemmatizer.lemmatize(words)\n",
    "# #     for word in root_words:\n",
    "#             freq_count_limit[limit_words.lower()]+=1\n",
    "# freq_count_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_count_limit.plot(20,cumulative=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_count_limit.pop(\"nt\")\n",
    "# freq_count_limit.pop(\"would\")\n",
    "# freq_count_limit.pop(\"one\")\n",
    "# freq_count_limit.pop(\"get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_count_limit.plot(20,cumulative=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists = top_100_words\n",
    "# x, y = zip(*lists)\n",
    "# x=list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-z]+')\n",
    "cv = CountVectorizer(ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "text_counts2 = cv.fit_transform(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 57416)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts2, df['subreddit'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.46875\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv (\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id                                            comment\n",
       "0          0                          Holy shit a shot counter.\n",
       "1          1  It doesn't matter that it isn't hard to rememb...\n",
       "2          2             I find it funny that this is downvoted\n",
       "3          3  They are really getting ridicoulous with all t...\n",
       "4          4                            He's Eden's best friend\n",
       "...      ...                                                ...\n",
       "19995  19995  These officials are almost as incompetent as o...\n",
       "19996  19996  honestly the Patriot act really fucked our com...\n",
       "19997  19997  My friend is now looking online for a thanos c...\n",
       "19998  19998  I really liked Thor Ragnarok and both Guardian...\n",
       "19999  19999                      last info changes everything.\n",
       "\n",
       "[20000 rows x 2 columns]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test = clean_text(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'like': 2896, 'would': 2317, 'nt': 2229, 'one': 2023, 'people': 1981, 'get': 1920, 'time': 1515, 'think': 1376, 'game': 1303, 'really': 1215, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = cleaned_test\n",
    "# filtered_sentence_test = [] \n",
    "# freq_count_limit_test = FreqDist()\n",
    "# lemmatizer_test=WordNetLemmatizer()\n",
    "# stop_words_test = set(stopwords.words('english'))\n",
    "\n",
    "# for i in d:\n",
    "#     comment_tokens_test = word_tokenize(i)\n",
    "#     for words in comment_tokens_test:\n",
    "#         if words not in stop_words_test: \n",
    "#             filtered_sentence_test.append(words) \n",
    "        \n",
    "#             limit_words_test = lemmatizer.lemmatize(words)\n",
    "# #     for word in root_words:\n",
    "#             freq_count_limit_test[limit_words_test.lower()]+=1\n",
    "# freq_count_limit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_test = RegexpTokenizer(r'[a-z]+')\n",
    "cv_test = CountVectorizer(ngram_range = (1,1), tokenizer = token_test.tokenize)\n",
    "text_counts_test = cv.fit_transform(cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     text_counts_test, df['subreddit'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Second Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.comment,df., test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# v = CountVectorizer()\n",
    "# X_train_count = v.fit_transform(X_train.values)\n",
    "# X_train_count.toarray()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# model = MultinomialNB()\n",
    "# model.fit(X_train,y_train)\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # noinspection SpellCheckingInspection\n",
    "# class NaiveBayesClassifier(object):\n",
    "#     def __init__(self, n_gram=1, printing=False):\n",
    "#         self.prior = defaultdict(int)\n",
    "#         self.logprior = {}\n",
    "#         self.bigdoc = defaultdict(list)\n",
    "#         self.loglikelihoods = defaultdict(defaultdict)\n",
    "#         self.V = []\n",
    "#         self.n = n_gram\n",
    "\n",
    "#         training_set = all_comments\n",
    "#         training_labels = df['subreddit']\n",
    "        \n",
    "#     def compute_prior_and_bigdoc(self, training_set, training_labels):\n",
    "        \n",
    "#         for x, y in zip(training_set, training_labels):\n",
    "#             all_words = x.split(\" \")\n",
    "#             if self.n == 1:\n",
    "#                 grams = all_words\n",
    "#             else:\n",
    "#                 grams = self.words_to_grams(all_words)\n",
    "\n",
    "#             self.prior[y] += len(grams)\n",
    "#             self.bigdoc[y].append(x)\n",
    "\n",
    "#     def compute_vocabulary(self, documents):\n",
    "#         vocabulary = set()\n",
    "\n",
    "#         for doc in documents:\n",
    "#             for word in doc.split(\" \"):\n",
    "#                 vocabulary.add(word.lower())\n",
    "\n",
    "#         return vocabulary\n",
    "\n",
    "#     def count_word_in_classes(self):\n",
    "#         counts = {}\n",
    "#         for c in list(self.bigdoc.keys()):\n",
    "#             docs = self.bigdoc[c]\n",
    "#             counts[c] = defaultdict(int)\n",
    "#             for doc in docs:\n",
    "#                 words = doc.split(\" \")\n",
    "#                 for word in words:\n",
    "#                     counts[c][word] += 1\n",
    "\n",
    "#         return counts\n",
    "\n",
    "#     def train(self, training_set, training_labels, alpha=1):\n",
    "#         # Get number of documents\n",
    "#         N_doc = len(training_set)\n",
    "\n",
    "#         # Get vocabulary used in training set\n",
    "#         self.V = self.compute_vocabulary(training_set)\n",
    "\n",
    "#         # Create bigdoc\n",
    "#         for x, y in zip(training_set, training_labels):\n",
    "#             self.bigdoc[y].append(x)\n",
    "\n",
    "#         # Get set of all classes\n",
    "#         all_classes = set(training_labels)\n",
    "\n",
    "#         # Compute a dictionary with all word counts for each class\n",
    "#         self.word_count = self.count_word_in_classes()\n",
    "\n",
    "#         # For each class\n",
    "#         for c in all_classes:\n",
    "#             # Get number of documents for that class\n",
    "#             N_c = float(sum(training_labels == c))\n",
    "\n",
    "#             # Compute logprior for class\n",
    "#             self.logprior[c] = np.log(N_c / N_doc)\n",
    "\n",
    "#             # Calculate the sum of counts of words in current class\n",
    "#             total_count = 0\n",
    "#             for word in self.V:\n",
    "#                 total_count += self.word_count[c][word]\n",
    "\n",
    "#             # For every word, get the count and compute the log-likelihood for this class\n",
    "#             for word in self.V:\n",
    "#                 count = self.word_count[c][word]\n",
    "#                 self.loglikelihoods[c][word] = np.log((count + alpha) / (total_count + alpha * len(self.V)))\n",
    "\n",
    "#     def predict(self, test_doc):\n",
    "#         sums = {\n",
    "#             0: 0,\n",
    "#             1: 0,\n",
    "#         }\n",
    "#         for c in self.bigdoc.keys():\n",
    "#             sums[c] = self.logprior[c]\n",
    "#             words = test_doc.split(\" \")\n",
    "#             for word in words:\n",
    "#                if word in self.V:\n",
    "#                    sums[c] += self.loglikelihoods[c][word]\n",
    "\n",
    "#         return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBclassifier = NaiveBayesClassifier(n_gram=1)\n",
    "# NBclassifier.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = NBclassifier.predict(test)\n",
    "# print(np.exp(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
